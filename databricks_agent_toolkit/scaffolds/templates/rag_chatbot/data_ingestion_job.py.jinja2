"""
Data Ingestion Job for RAG Pipeline.

Watches UC Volume for new documents, processes them, and stores embeddings in Delta.
Runs nightly via Databricks Job (configured in databricks.yml).

Official Databricks Pattern:
1. UC Volume ‚Üí Raw documents storage
2. ai_parse() ‚Üí Chunk documents
3. FMAPI ‚Üí Generate embeddings
4. Delta ‚Üí Store embeddings
5. Synced Table ‚Üí Auto-sync to Lakebase pgvector
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, TimestampType
from databricks.sdk import WorkspaceClient
import os


# Configuration from widgets/env
UC_VOLUME_PATH = dbutils.widgets.get("uc_volume_path") if 'dbutils' in globals() else os.getenv("UC_VOLUME_PATH", "/Volumes/catalog/schema/docs")
DELTA_TABLE = dbutils.widgets.get("delta_table") if 'dbutils' in globals() else os.getenv("DELTA_TABLE", "catalog.schema.embeddings")
EMBEDDING_MODEL = dbutils.widgets.get("embedding_model") if 'dbutils' in globals() else os.getenv("EMBEDDING_MODEL", "databricks-bge-large-en")
CHUNK_SIZE = int(dbutils.widgets.get("chunk_size") if 'dbutils' in globals() else os.getenv("CHUNK_SIZE", "512"))
CHUNK_OVERLAP = int(dbutils.widgets.get("chunk_overlap") if 'dbutils' in globals() else os.getenv("CHUNK_OVERLAP", "50"))


print("üöÄ Data Ingestion Job Started")
print(f"   UC Volume: {UC_VOLUME_PATH}")
print(f"   Delta Table: {DELTA_TABLE}")
print(f"   Embedding Model: {EMBEDDING_MODEL}")
print(f"   Chunk Size: {CHUNK_SIZE}")


def setup_delta_table(spark: SparkSession, table_name: str):
    """
    Create Delta table for storing embeddings if it doesn't exist.
    
    Schema:
    - id: Unique identifier (auto-generated)
    - file_path: Source file path in UC Volume
    - content: Text chunk
    - embedding: Vector embedding (array<float>)
    - chunk_index: Chunk position in document
    - total_chunks: Total chunks in document
    - metadata: Additional metadata (JSON)
    - created_at: Timestamp
    """
    schema = StructType([
        StructField("id", StringType(), False),
        StructField("file_path", StringType(), False),
        StructField("content", StringType(), False),
        StructField("embedding", ArrayType(FloatType()), False),
        StructField("chunk_index", StringType(), True),
        StructField("total_chunks", StringType(), True),
        StructField("metadata", StringType(), True),  # JSON string
        StructField("created_at", TimestampType(), False),
    ])
    
    # Check if table exists
    if not spark.catalog._jcatalog.tableExists(table_name):
        print(f"üìä Creating Delta table: {table_name}")
        
        # Create empty DataFrame with schema
        df = spark.createDataFrame([], schema)
        
        # Write as Delta table with Change Data Feed enabled (required for Synced Table)
        (df.write
         .format("delta")
         .mode("overwrite")
         .option("delta.enableChangeDataFeed", "true")
         .saveAsTable(table_name))
        
        print(f"‚úÖ Created Delta table with CDF enabled")
    else:
        print(f"‚úÖ Delta table exists: {table_name}")


def discover_new_documents(spark: SparkSession, volume_path: str, processed_files_table: str = None):
    """
    Discover new documents in UC Volume.
    
    Args:
        volume_path: Path to UC Volume (e.g., /Volumes/catalog/schema/docs)
        processed_files_table: Optional table tracking processed files
    
    Returns:
        List of file paths to process
    """
    print(f"\n1Ô∏è‚É£ Discovering documents in {volume_path}...")
    
    # List all files in UC Volume
    # Databricks AutoLoader for incremental processing
    df_files = (spark.read
                .format("binaryFile")
                .option("pathGlobFilter", "*.{txt,pdf,md,doc,docx}")
                .option("recursiveFileLookup", "true")
                .load(volume_path))
    
    file_count = df_files.count()
    print(f"   Found {file_count} documents")
    
    # TODO: Filter out already processed files
    # For now, process all files (idempotent with upsert)
    
    return df_files


def chunk_documents_with_ai_parse(spark: SparkSession, df_files):
    """
    Chunk documents using ai_parse() - Databricks built-in function.
    
    ai_parse() automatically:
    - Extracts text from PDFs, Word docs, etc.
    - Chunks intelligently based on document structure
    - Preserves metadata
    
    Reference: https://docs.databricks.com/sql/language-manual/functions/ai_parse.html
    """
    print(f"\n2Ô∏è‚É£ Chunking documents with ai_parse()...")
    
    # Use ai_parse for intelligent document parsing and chunking
    df_chunks = df_files.selectExpr(
        "path as file_path",
        f"ai_parse(content, 'chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}') as chunks",
        "modificationTime"
    )
    
    # Explode chunks array to individual rows
    df_exploded = (df_chunks
                   .select(
                       F.col("file_path"),
                       F.posexplode(F.col("chunks")).alias("chunk_index", "chunk"),
                       F.col("modificationTime")
                   )
                   .select(
                       F.col("file_path"),
                       F.col("chunk.text").alias("content"),
                       F.col("chunk_index"),
                       F.size(F.col("chunks")).alias("total_chunks"),
                       F.col("chunk.metadata").alias("metadata"),
                       F.col("modificationTime")
                   ))
    
    chunk_count = df_exploded.count()
    print(f"   Generated {chunk_count} chunks")
    
    return df_exploded


def generate_embeddings_with_fmapi(spark: SparkSession, df_chunks, embedding_model: str):
    """
    Generate embeddings using Foundation Model APIs (FMAPI).
    
    Uses Databricks SQL function ai_query() with embedding model.
    This is more efficient than UDFs as it's vectorized.
    
    Reference: https://docs.databricks.com/sql/language-manual/functions/ai_query.html
    """
    print(f"\n3Ô∏è‚É£ Generating embeddings with {embedding_model}...")
    
    # Use ai_query for embedding generation (vectorized, efficient)
    df_embeddings = df_chunks.selectExpr(
        "uuid() as id",
        "file_path",
        "content",
        f"ai_query('{embedding_model}', content) as embedding",  # FMAPI embedding call
        "cast(chunk_index as string) as chunk_index",
        "cast(total_chunks as string) as total_chunks",
        "to_json(struct(*)) as metadata",  # Convert metadata to JSON
        "current_timestamp() as created_at"
    )
    
    # Filter out failed embeddings
    df_embeddings = df_embeddings.filter(F.col("embedding").isNotNull())
    
    embedding_count = df_embeddings.count()
    print(f"   Generated {embedding_count} embeddings")
    
    return df_embeddings


def store_in_delta(spark: SparkSession, df_embeddings, delta_table: str):
    """
    Store embeddings in Delta table with upsert (merge).
    
    Uses Delta MERGE for idempotent writes - safe to rerun.
    """
    print(f"\n4Ô∏è‚É£ Storing embeddings in {delta_table}...")
    
    # Write to Delta with merge (upsert) for idempotency
    (df_embeddings.write
     .format("delta")
     .mode("append")  # Append new records
     .option("mergeSchema", "true")  # Allow schema evolution
     .saveAsTable(delta_table))
    
    print(f"‚úÖ Stored {df_embeddings.count()} embeddings in Delta")


def main():
    """Main ingestion pipeline."""
    spark = SparkSession.builder.getOrCreate()
    
    print("\n" + "="*70)
    print("  RAG DATA INGESTION PIPELINE")
    print("="*70)
    
    # Step 0: Setup Delta table
    setup_delta_table(spark, DELTA_TABLE)
    
    # Step 1: Discover new documents in UC Volume
    df_files = discover_new_documents(spark, UC_VOLUME_PATH)
    
    if df_files.count() == 0:
        print("\n‚úÖ No new documents to process")
        return
    
    # Step 2: Chunk documents with ai_parse
    df_chunks = chunk_documents_with_ai_parse(spark, df_files)
    
    # Step 3: Generate embeddings with FMAPI
    df_embeddings = generate_embeddings_with_fmapi(spark, df_chunks, EMBEDDING_MODEL)
    
    # Step 4: Store in Delta table
    store_in_delta(spark, df_embeddings, DELTA_TABLE)
    
    print("\n" + "="*70)
    print("‚úÖ INGESTION COMPLETE")
    print("="*70)
    print(f"\nüìä Summary:")
    print(f"   - Documents processed: {df_files.count()}")
    print(f"   - Chunks generated: {df_chunks.count()}")
    print(f"   - Embeddings stored: {df_embeddings.count()}")
    print(f"   - Delta table: {DELTA_TABLE}")
    print(f"\nüìã Next: Synced Table will auto-sync to Lakebase")


if __name__ == "__main__":
    main()
