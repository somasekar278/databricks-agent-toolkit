"""
Embedding generation using Databricks Foundation Model APIs.

Supports multiple embedding models via Model Serving endpoints.
"""

import os
from typing import List, Optional
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import ChatMessage, ChatMessageRole


class EmbeddingGenerator:
    """Generate embeddings using Databricks Foundation Model APIs."""

    def __init__(
        self,
        model_endpoint: Optional[str] = None,
        workspace_client: Optional[WorkspaceClient] = None,
    ):
        """
        Initialize embedding generator.

        Args:
            model_endpoint: Embedding model endpoint (default: databricks-bge-large-en)
            workspace_client: Databricks WorkspaceClient (auto-created if None)
        """
        self.model_endpoint = model_endpoint or os.getenv(
            "EMBEDDING_ENDPOINT", "databricks-bge-large-en"
        )
        self.client = workspace_client or WorkspaceClient()

        print(f"âœ… Embedding generator initialized: {self.model_endpoint}")

    def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Input text to embed

        Returns:
            Embedding vector as list of floats
        """
        try:
            response = self.client.serving_endpoints.query(
                name=self.model_endpoint,
                messages=[
                    ChatMessage(role=ChatMessageRole.USER, content=text)
                ],
            )

            # Extract embedding from response
            if hasattr(response, "data") and response.data:
                return response.data[0].embedding
            elif hasattr(response, "embeddings") and response.embeddings:
                return response.embeddings[0]
            else:
                raise ValueError(f"Unexpected response format: {response}")

        except Exception as e:
            raise RuntimeError(f"Failed to generate embedding: {e}")

    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        Generate embeddings for multiple texts (batched).

        Args:
            texts: List of input texts
            batch_size: Number of texts to process per batch

        Returns:
            List of embedding vectors
        """
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            print(f"ðŸ”„ Processing batch {i // batch_size + 1}/{(len(texts) + batch_size - 1) // batch_size}")

            for text in batch:
                embedding = self.generate_embedding(text)
                embeddings.append(embedding)

        print(f"âœ… Generated {len(embeddings)} embeddings")
        return embeddings


def chunk_text(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
    """
    Split text into overlapping chunks for indexing.

    Args:
        text: Input text to chunk
        chunk_size: Maximum characters per chunk
        overlap: Number of overlapping characters between chunks

    Returns:
        List of text chunks
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]

        # Try to break at sentence boundary
        if end < len(text):
            last_period = chunk.rfind(".")
            last_newline = chunk.rfind("\n")
            break_point = max(last_period, last_newline)

            if break_point > chunk_size // 2:  # Only break if it's reasonable
                chunk = text[start : start + break_point + 1]
                end = start + break_point + 1

        chunks.append(chunk.strip())
        start = end - overlap

    return [c for c in chunks if c]  # Filter empty chunks
