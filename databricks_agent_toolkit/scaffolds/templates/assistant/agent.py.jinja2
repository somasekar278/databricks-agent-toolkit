"""
{{ name }} - Assistant Agent with Memory (OpenAI API Compatible)
Generated by: Databricks Agent Toolkit v0.2.0
"""
import os
import yaml
import uuid
from pathlib import Path
from typing import AsyncIterator, List, Dict, Any, Optional
from databricks_agent_toolkit.integrations import DatabricksLLM
import mlflow
from memory_manager import MemoryManager

# Load config
config = yaml.safe_load((Path(__file__).parent / "config.yaml").read_text())

# Setup MLflow
mlflow.set_tracking_uri("databricks")
try:
    mlflow.set_experiment(config["mlflow"]["experiment"])
    print(f"üìä MLflow: {config['mlflow']['experiment']}")
except Exception as e:
    print(f"‚ö†Ô∏è  MLflow setup: {e}")


class AssistantAgent:
    """
    OpenAI API compatible assistant with conversation memory.

    Stores conversation history in Lakebase (Databricks PostgreSQL).
    """

    def __init__(self):
        self.llm = DatabricksLLM(
            endpoint=config["model"]["endpoint"],
            auto_trace=config["mlflow"].get("auto_trace", True)
        )
        self.memory = MemoryManager(
            host=config["memory"].get("host"),
            database=config["memory"].get("database"),
            user=config["memory"].get("user"),
            password=config["memory"].get("password"),
            max_history=config["memory"].get("max_history", 20)
        )
        self.memory.initialize()
        self.config = config
        self.system_prompt = config["model"].get(
            "system_prompt",
            "You are a helpful assistant with conversation memory."
        )
        print(f"ü§ñ Agent initialized: {config['model']['endpoint']}")
        print(f"üíæ Memory: Lakebase")

    def predict(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        session_id: Optional[str] = None
    ):
        """
        OpenAI API compatible prediction with memory.

        Args:
            messages: List of {role, content} dicts
            stream: Whether to stream response
            session_id: Conversation session ID

        Returns:
            OpenAI format response (coroutine) or async generator
        """
        # Generate session if not provided
        if not session_id:
            session_id = str(uuid.uuid4())

        # Extract user message (last message in input)
        user_message = None
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break

        if user_message:
            # Store user message in memory
            self.memory.store_message(session_id, "user", user_message)

        # Get conversation history
        history = self.memory.get_messages_for_llm(session_id)

        # Build messages with system prompt + history
        full_messages = [
            {"role": "system", "content": self.system_prompt},
            *history
        ]

        if stream:
            return self._stream_response(full_messages, session_id)
        else:
            return self._generate_response(full_messages, session_id)

    async def _generate_response(
        self,
        messages: List[Dict[str, str]],
        session_id: str
    ) -> Dict[str, Any]:
        """Non-streaming response (OpenAI format)"""
        try:
            result = await self.llm.chat(
                messages=messages,
                temperature=self.config["model"].get("temperature", 0.7),
                max_tokens=self.config["model"].get("max_tokens", 500)
            )

            response_content = result["content"]

            # Store assistant response in memory
            self.memory.store_message(session_id, "assistant", response_content)

            # Return OpenAI Responses API format
            return {
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_content
                    },
                    "finish_reason": "stop"
                }],
                "session_id": session_id
            }
        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            raise

    async def _stream_response(
        self,
        messages: List[Dict[str, str]],
        session_id: str
    ) -> AsyncIterator[Dict[str, Any]]:
        """Streaming response (OpenAI format)"""
        accumulated_content = ""
        try:
            async for chunk in self.llm.stream(
                messages=messages,
                temperature=self.config["model"].get("temperature", 0.7),
                max_tokens=self.config["model"].get("max_tokens", 500)
            ):
                if chunk and chunk.strip():
                    accumulated_content += chunk
                    # OpenAI streaming format
                    yield {
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }],
                        "session_id": session_id
                    }

            # Store complete response in memory
            if accumulated_content:
                self.memory.store_message(session_id, "assistant", accumulated_content)

            # Final chunk
            yield {
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }],
                "session_id": session_id
            }
        except Exception as e:
            print(f"‚ùå Streaming error: {e}")
            raise

    def clear_session(self, session_id: str):
        """Clear conversation history for a session"""
        self.memory.clear_conversation(session_id)


# Create agent instance
agent = AssistantAgent()
