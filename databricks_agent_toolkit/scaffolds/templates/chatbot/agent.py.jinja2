"""
{{ name }} - Chatbot Agent (OpenAI API Compatible)
Generated by: Databricks Agent Toolkit v0.2.0
"""
import os
import yaml
import asyncio
from pathlib import Path
from typing import AsyncIterator, List, Dict, Any, Optional
import mlflow
from mlflow.deployments import get_deploy_client

# Load config
config = yaml.safe_load((Path(__file__).parent / "config.yaml").read_text())

# Setup MLflow
mlflow.set_tracking_uri("databricks")
try:
    mlflow.set_experiment(config["mlflow"]["experiment"])
    print(f"üìä MLflow: {config['mlflow']['experiment']}")
except Exception as e:
    print(f"‚ö†Ô∏è  MLflow setup: {e}")


# Simple DatabricksLLM implementation (inline to avoid toolkit dependency)
class DatabricksLLM:
    """Simple LLM client for Databricks Model Serving"""

    def __init__(self, endpoint: str, auto_trace: bool = True, stream_delay_ms: float = 50):
        self.endpoint = endpoint
        self.client = get_deploy_client("databricks")
        self.auto_trace = auto_trace
        self.stream_delay_ms = stream_delay_ms / 1000.0  # Convert ms to seconds

    async def chat(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 500) -> Dict:
        """Non-streaming chat completion"""
        response = self.client.predict(
            endpoint=self.endpoint,
            inputs={"messages": messages, "temperature": temperature, "max_tokens": max_tokens}
        )
        return {"content": response["choices"][0]["message"]["content"]}

    async def stream(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 500) -> AsyncIterator[str]:
        """Streaming chat completion"""
        # Note: MLflow client doesn't support streaming in this simple implementation
        # Fall back to non-streaming and simulate streaming
        result = await self.chat(messages, temperature, max_tokens)
        content = result["content"]
        # Stream with configurable delay
        for word in content.split():
            yield word + " "
            await asyncio.sleep(self.stream_delay_ms)


class ChatbotAgent:
    """
    OpenAI API compatible chatbot agent.

    Follows MLflow ResponsesAgent interface for Databricks Apps.
    """

    def __init__(self):
        self.llm = DatabricksLLM(
            endpoint=config["model"]["endpoint"],
            auto_trace=config["mlflow"].get("auto_trace", True),
            stream_delay_ms=config["model"].get("token_delay_ms", 50)
        )
        self.config = config
        self.system_prompt = config["model"].get(
            "system_prompt",
            "You are a helpful AI assistant."
        )
        print(f"ü§ñ Agent initialized: {config['model']['endpoint']}")

    def predict(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False
    ):
        """
        OpenAI API compatible prediction.

        Args:
            messages: List of {role, content} dicts
            stream: Whether to stream response

        Returns:
            OpenAI format response (coroutine) or async generator
        """
        # Prepend system prompt if not present
        if not messages or messages[0].get("role") != "system":
            messages = [
                {"role": "system", "content": self.system_prompt},
                *messages
            ]

        if stream:
            return self._stream_response(messages)
        else:
            return self._generate_response(messages)

    async def _generate_response(
        self,
        messages: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """Non-streaming response (OpenAI format)"""
        try:
            result = await self.llm.chat(
                messages=messages,
                temperature=self.config["model"].get("temperature", 0.7),
                max_tokens=self.config["model"].get("max_tokens", 500)
            )

            # Return OpenAI Responses API format
            return {
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["content"]
                    },
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            raise

    async def _stream_response(
        self,
        messages: List[Dict[str, str]]
    ) -> AsyncIterator[Dict[str, Any]]:
        """Streaming response (OpenAI format)"""
        try:
            async for chunk in self.llm.stream(
                messages=messages,
                temperature=self.config["model"].get("temperature", 0.7),
                max_tokens=self.config["model"].get("max_tokens", 500)
            ):
                if chunk and chunk.strip():
                    # OpenAI streaming format
                    yield {
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk},
                            "finish_reason": None
                        }]
                    }

            # Final chunk
            yield {
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            print(f"‚ùå Streaming error: {e}")
            raise


# Create agent instance
agent = ChatbotAgent()
