"""
e2e-test-l1-fixed - L1 Simple Chatbot
Generated by: Databricks Agent Toolkit v0.1.0

Learning goals:
- Use Databricks Model Serving via toolkit
- Automatic MLflow tracing
- Basic conversation handling
- Deploy to Databricks Apps

Next steps: Generate L2 to add memory
"""

from databricks_agent_toolkit.integrations import DatabricksLLM
import mlflow
import asyncio
import yaml
from pathlib import Path
from typing import Optional

# Load configuration
config_path = Path(__file__).parent / "config.yaml"
with open(config_path) as f:
    config = yaml.safe_load(f)

# Configuration
ENABLE_STREAMING = config.get("model", {}).get("streaming", False)
SHOW_MLFLOW_URL = config.get("mlflow", {}).get("show_url", False)
ASSESSMENTS_ENABLED = config.get("mlflow", {}).get("assessments", {}).get("enabled", True)
ASSESSMENT_FREQUENCY = config.get("mlflow", {}).get("assessments", {}).get("frequency", 5)

# Initialize toolkit integration (handles auth, tracing automatically)
llm = DatabricksLLM(
    endpoint="databricks-claude-sonnet-4-5",  # Modern foundation model
    auto_trace=True  # Automatic MLflow tracing
)

async def chat_batch(user_message: str) -> str:
    """
    Batch mode: Get complete response at once.
    
    Use when: You need the full response before displaying
    TODO: Customize the system prompt below
    """
    response = await llm.chat(
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."  # TODO: Customize this
            },
            {
                "role": "user",
                "content": user_message
            }
        ],
        temperature=0.7,
        max_tokens=500
    )
    
    return response["content"]


async def chat_stream(user_message: str):
    """
    Streaming mode: Get response token-by-token.
    
    Use when: You want real-time display as model generates
    Yields: Text chunks as they arrive
    """
    async for chunk in llm.stream(
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant."  # TODO: Customize this
            },
            {
                "role": "user",
                "content": user_message
            }
        ],
        temperature=0.7,
        max_tokens=500
    ):
        yield chunk


def collect_assessment() -> Optional[dict]:
    """
    Collect human feedback/assessment.
    
    Returns dict with rating and optional comment, or None if skipped.
    """
    print("\n" + "="*50)
    print("ðŸ“Š Quick Feedback (helps improve the bot)")
    print("="*50)
    
    try:
        rating_input = input("Rate this conversation (1-5, or press Enter to skip): ").strip()
        
        if not rating_input:
            return None
        
        rating = int(rating_input)
        if rating < 1 or rating > 5:
            print("âš ï¸  Invalid rating, skipping")
            return None
        
        comment = input("Any comments? (optional): ").strip()
        
        return {
            "rating": rating,
            "comment": comment if comment else None
        }
    
    except (ValueError, KeyboardInterrupt):
        return None


def log_assessment_to_mlflow(assessment: dict, run_id: Optional[str] = None):
    """
    Log assessment to MLflow.
    
    This creates an Assessment in MLflow UI for review and analysis.
    """
    try:
        # In MLflow 3+, assessments are logged as tags/metrics
        if run_id:
            with mlflow.start_run(run_id=run_id):
                mlflow.log_metric("user_rating", assessment["rating"])
                if assessment.get("comment"):
                    mlflow.set_tag("user_feedback", assessment["comment"])
        else:
            # Log to active run if any
            mlflow.log_metric("user_rating", assessment["rating"])
            if assessment.get("comment"):
                mlflow.set_tag("user_feedback", assessment["comment"])
        
        print("âœ… Feedback recorded - thank you!")
    
    except Exception as e:
        print(f"âš ï¸  Could not log feedback: {e}")


# Simple CLI interface for testing
async def main():
    print("ðŸ¤– e2e-test-l1-fixed (Chatbot)")
    print(f"Mode: {'Streaming' if ENABLE_STREAMING else 'Batch'}")
    print("Type 'quit' to exit\n")
    
    # Set up MLflow tracking
    mlflow.set_tracking_uri("databricks")
    experiment_path = config.get("mlflow", {}).get("experiment", "/Shared/e2e-test-l1-fixed")
    
    try:
        mlflow.set_experiment(experiment_path)
        experiment = mlflow.get_experiment_by_name(experiment_path)
        print(f"ðŸ“Š MLflow: {experiment_path}")
    except Exception as e:
        print(f"âš ï¸  MLflow setup: {e}")
        experiment = None
    
    print()
    
    conversation_count = 0
    
    # Start MLflow run for the session
    with mlflow.start_run(run_name="chat_session") as run:
        while True:
            user_input = input("You: ")
            if user_input.lower() == 'quit':
                break
            
            print("Bot: ", end="", flush=True)
            
            if ENABLE_STREAMING:
                # Streaming mode: Display tokens as they arrive
                async for chunk in chat_stream(user_input):
                    print(chunk, end="", flush=True)
                print("\n")
            else:
                # Batch mode: Display complete response
                response = await chat_batch(user_input)
                print(f"{response}\n")
            
            conversation_count += 1
            
            # Periodically ask for feedback
            if (ASSESSMENTS_ENABLED and 
                ASSESSMENT_FREQUENCY > 0 and 
                conversation_count % ASSESSMENT_FREQUENCY == 0):
                
                assessment = collect_assessment()
                if assessment:
                    log_assessment_to_mlflow(assessment, run.info.run_id)
                print()
        
        # Show friendly end message
        print("\n" + "="*60)
        print(f"ðŸ‘‹ Thanks for chatting!")
        print(f"   {conversation_count} conversation(s) logged for analysis")
        
        # Optionally show technical MLflow URL (for developers)
        if SHOW_MLFLOW_URL and experiment:
            try:
                from databricks.sdk import WorkspaceClient
                w = WorkspaceClient()
                host = w.config.host.rstrip('/')
                mlflow_url = f"{host}/ml/experiments/{experiment.experiment_id}/runs/{run.info.run_id}"
                
                print()
                print("ðŸ”§ Developer Info:")
                print(f"   MLflow Run: {mlflow_url}")
            except Exception:
                pass
        
        print("="*60)


if __name__ == "__main__":
    asyncio.run(main())
